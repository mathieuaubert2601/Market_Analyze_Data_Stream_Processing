# ðŸ“ˆ Market Analyze Data Stream Processing

A real-time **Financial Market Analysis System** powered by **Apache Kafka**, **RAG (Retrieval-Augmented Generation)**, and **LLM** (Llama 3.3 70B via Groq). This project ingests live market data, financial news, and technical indicators, then provides AI-powered insights through an interactive Streamlit dashboard.

Acces to the streamlit live of our application (the data are those published on the github, there is no update because streamlit live do not allow to run kafka) https://marketanalyzedatastreamprocessing-2gecwcwivwcr54mfuqrjmh.streamlit.app/

---

## ðŸ“‘ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Features](#features)
- [Project Structure](#project-structure)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Configuration](#configuration)
- [Quick Start](#quick-start)
- [Usage](#usage)
  - [Producer](#producer)
  - [Consumer](#consumer)
  - [Streamlit App](#streamlit-app)
- [Kafka Topics](#kafka-topics)
- [Technical Details](#technical-details)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)
- [License](#license)

---

## Overview

**Market Analyze Data Stream Processing** is a real-time financial analytics platform that: 

1. **Ingests** live market data from Yahoo Finance and Google News RSS
2. **Processes** data through Apache Kafka streams with sentiment analysis
3. **Stores** enriched documents in ChromaDB vector database for semantic search
4. **Analyzes** user queries using RAG with Llama 3.3 70B LLM
5. **Visualizes** insights through an interactive Streamlit dashboard

The system monitors **10 major CAC 40 stocks** (LVMH, TotalEnergies, L'OrÃ©al, HermÃ¨s, Sanofi, Safran, Schneider Electric, Air Liquide, BNP Paribas, Vinci) and provides institutional-grade market analysis. 

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           DATA SOURCES                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Yahoo Finance API          Google News RSS                                  â”‚
â”‚  â€¢ Real-time prices         â€¢ Financial news articles                        â”‚
â”‚  â€¢ Historical OHLCV         â€¢ Company-specific news                          â”‚
â”‚  â€¢ Company info             â€¢ Multi-language support                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         KAFKA PRODUCER                                       â”‚
â”‚  src/ingestion/producer.py                                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Fetches live market data (price, volume, MA indicators)                   â”‚
â”‚  â€¢ Scrapes Google News RSS for financial news                                â”‚
â”‚  â€¢ Generates intraday metrics (10m, 30m, 1h, 3h, 6h momentum)                â”‚
â”‚  â€¢ Calculates technical analysis (MA10, MA50, MA200, trend)                  â”‚
â”‚  â€¢ Creates daily summaries (OHLCV + variation)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         APACHE KAFKA                                         â”‚
â”‚  docker-compose.yml (Zookeeper + Kafka)                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Topics:                                                                     â”‚
â”‚  â€¢ financial-news      â†’ News articles + Technical analysis                  â”‚
â”‚  â€¢ stock-history       â†’ OHLCV history for charts                            â”‚
â”‚  â€¢ hot-news-events     â†’ Intraday metrics & momentum                         â”‚
â”‚  â€¢ daily-summary       â†’ End-of-day summaries                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         KAFKA CONSUMER                                       â”‚
â”‚  src/processing/consumer.py                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Translates news to English (deep-translator)                              â”‚
â”‚  â€¢ Performs sentiment analysis (VADER)                                       â”‚
â”‚  â€¢ Generates embeddings (sentence-transformers)                              â”‚
â”‚  â€¢ Stores in ChromaDB vector database                                        â”‚
â”‚  â€¢ Saves OHLCV history to CSV files                                          â”‚
â”‚  â€¢ Enforces 30-day data retention policy                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CHROMADB + CSV                                       â”‚
â”‚  data/chromadb/ + data/history/                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Vector embeddings for semantic search                                     â”‚
â”‚  â€¢ Metadata:  ticker, timestamp, sentiment, prices, MA indicators             â”‚
â”‚  â€¢ CSV files for candlestick charts                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RAG ENGINE                                           â”‚
â”‚  src/app/rag_engine.py                                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Query Router Agent (intent detection + time window calculation)           â”‚
â”‚  â€¢ Semantic search in ChromaDB                                               â”‚
â”‚  â€¢ Re-ranking with time decay for real-time queries                          â”‚
â”‚  â€¢ Context building for LLM                                                  â”‚
â”‚  â€¢ Llama 3.3 70B (Groq API) for response generation                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         STREAMLIT APP                                        â”‚
â”‚  src/app/main.py                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Real-time market watch sidebar                                            â”‚
â”‚  â€¢ AI-powered market analyst chat interface                                  â”‚
â”‚  â€¢ Interactive candlestick charts (MA50 + MA200)                             â”‚
â”‚  â€¢ Source context display (news, technicals, daily summaries)                â”‚
â”‚  â€¢ Pipeline health monitoring                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Features

### ðŸ”„ Real-Time Data Ingestion
- **Yahoo Finance Integration**: Live prices, OHLCV data, company info
- **Google News RSS**:  Financial news scraping with multi-language support
- **Intraday Momentum**: 10min, 30min, 1h, 3h, 6h price variations
- **Technical Indicators**: MA10, MA50, MA200, trend detection (Bullish/Bearish/Neutral)

### ðŸ§  AI-Powered Analysis
- **RAG Architecture**: Retrieval-Augmented Generation for context-aware responses
- **Query Router Agent**:  Automatic intent detection (REAL_TIME vs HISTORICAL)
- **Time-Aware Search**: Dynamic time windows based on user queries
- **Sentiment Analysis**: VADER sentiment scoring on translated news
- **LLM Integration**: Llama 3.3 70B via Groq API for institutional-grade analysis

### ðŸ“Š Interactive Dashboard
- **Market Watch Sidebar**: Real-time prices with delta indicators
- **Candlestick Charts**: Interactive Plotly charts with MA overlays
- **Context Sources Panel**: Display retrieved documents with metadata
- **Pipeline Health Monitor**: Real-time consumer status monitoring

### ðŸ—ƒï¸ Data Management
- **Vector Database**: ChromaDB for semantic search
- **Data Retention**:  Automatic 30-day cleanup policy
- **Deduplication**: Unique ID-based deduplication for news and metrics

---

## Project Structure

```
Market_Analyze_Data_Stream_Processing/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config. py                 # Global configuration (Kafka, tickers, paths)
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â””â”€â”€ producer.py           # Kafka producer (data fetching)
â”‚   â”œâ”€â”€ processing/
â”‚   â”‚   â””â”€â”€ consumer.py           # Kafka consumer (data processing + storage)
â”‚   â””â”€â”€ app/
â”‚       â”œâ”€â”€ main.py               # Streamlit dashboard
â”‚       â””â”€â”€ rag_engine.py         # RAG logic + LLM integration
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ chromadb/                 # Vector database storage
â”‚   â””â”€â”€ history/                  # CSV files for charts
â”œâ”€â”€ docker-compose.yml            # Kafka + Zookeeper infrastructure
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ start.sh                      # Linux/macOS startup script
â”œâ”€â”€ start.bat                     # Windows startup script
â”œâ”€â”€ . env                          # Environment variables (API keys)
â””â”€â”€ README.md                     # This file
```

---

## Prerequisites

- **Python** 3.10+
- **Docker** & **Docker Compose**
- **Conda** (Miniconda/Anaconda) - recommended for environment management
- **Groq API Key** (free tier available at [console.groq.com](https://console.groq.com))

---

## Installation

### 1. Clone the Repository

```bash
git clone https://github.com/mathieuaubert2601/Market_Analyze_Data_Stream_Processing.git
cd Market_Analyze_Data_Stream_Processing
```

### 2. Create Conda Environment

```bash
conda create -n dsp-project python=3.10 -y
conda activate dsp-project
```

### 3. Install Dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### 4. Start Kafka Infrastructure

```bash
docker compose up -d
```

Wait for Kafka to be ready (port 9092):

```bash
# Linux/macOS
while ! nc -z localhost 9092; do sleep 1; done && echo "Kafka is ready!"

# Windows PowerShell
while (!(Test-NetConnection localhost -Port 9092).TcpTestSucceeded) { Start-Sleep 1 }; Write-Host "Kafka is ready!"
```

---

## Configuration

Create a `.env` file in the project root:

```env
GROQ_API_KEY=your_groq_api_key_here
```

### Configuration Options (src/config.py)

| Variable | Description | Default |
|----------|-------------|---------|
| `KAFKA_HOST` | Kafka broker address | `localhost:9092` |
| `TICKERS` | List of stock symbols to monitor | CAC 40 top 10 |
| `SLEEP_TIME` | Interval between data fetches (seconds) | `60` |
| `CHROMA_PATH` | ChromaDB storage path | `./data/chromadb` |
| `HISTORY_PATH` | CSV history storage path | `./data/history` |
| `EMBEDDING_MODEL_NAME` | Sentence transformer model | `all-MiniLM-L6-v2` |
| `LLM_MODEL_NAME` | Groq LLM model | `llama-3.3-70b-versatile` |

### Monitored Stocks

| Ticker | Company |
|--------|---------|
| MC. PA | LVMH MoÃ«t Hennessy |
| TTE.PA | TotalEnergies |
| OR.PA | L'OrÃ©al |
| RMS.PA | HermÃ¨s International |
| SAN.PA | Sanofi |
| SAF.PA | Safran |
| SU.PA | Schneider Electric |
| AI.PA | Air Liquide |
| BNP.PA | BNP Paribas |
| DG.PA | Vinci |

---

## Quick Start

### Option 1: Automated Scripts

**Linux/macOS (WSL):**
```bash
chmod +x start.sh
./start.sh
```

**Windows:**
```batch
start.bat
```

### Option 2: Manual Start

```bash
# Terminal 1: Start Kafka
docker compose up -d

# Terminal 2: Start Producer
conda activate dsp-project
python src/ingestion/producer.py

# Terminal 3: Start Consumer (wait ~10 seconds after producer)
conda activate dsp-project
python src/processing/consumer.py

# Terminal 4: Start Streamlit App (wait ~10 minutes for initial data)
conda activate dsp-project
streamlit run src/app/main. py
```

Access the dashboard at:  **http://localhost:8501**

---

## Usage

### Producer

The producer fetches data every 60 seconds and sends it to Kafka:

```python
# Data sources: 
# 1. Google News RSS - Financial news for each ticker
# 2. Yahoo Finance - Real-time prices, history, company info

# Generated data types:
# â€¢ News articles (from Yahoo API + Google RSS)
# â€¢ Intraday metrics (10m, 30m, 1h, 3h, 6h momentum)
# â€¢ Technical analysis (MA10, MA50, MA200, trend)
# â€¢ Daily summaries (OHLCV + daily variation)
# â€¢ History data (for candlestick charts)
```

### Consumer

The consumer processes messages from all Kafka topics:

```python
# Processing pipeline:
# 1. Translate news to English (deep-translator)
# 2. Compute sentiment score (VADER)
# 3. Generate embeddings (sentence-transformers)
# 4. Store in ChromaDB with metadata
# 5. Save OHLCV to CSV files
# 6. Enforce 30-day retention policy
```

### Streamlit App

The dashboard provides:

1. **Market Watch Sidebar**
   - Real-time prices with delta percentages
   - Market state indicator (ðŸŸ¢ Open / ðŸ”´ Closed)
   - Last update timestamp

2. **AI Market Analyst**
   - Natural language queries about market trends
   - Automatic ticker detection from context
   - Time-aware responses (real-time vs historical)

3. **Candlestick Charts**
   - Interactive Plotly charts
   - MA50 and MA200 overlays
   - Auto-updates from Kafka stream

4. **Context Sources**
   - Retrieved documents with metadata
   - Sentiment scores
   - Direct links to original sources

### Example Queries

```
â€¢ "Why is LVMH dropping today?"
â€¢ "What happened to TotalEnergies last week?"
â€¢ "Give me a technical analysis of HermÃ¨s"
â€¢ "What's the sentiment around BNP Paribas?"
â€¢ "Compare Air Liquide and Schneider Electric performance"
```

---

## Kafka Topics

| Topic | Description | Data Type |
|-------|-------------|-----------|
| `financial-news` | News articles + Technical analysis | JSON |
| `stock-history` | OHLCV data for charts | JSON |
| `hot-news-events` | Intraday metrics & momentum | JSON |
| `daily-summary` | End-of-day summaries | JSON |

---

## Technical Details

### RAG Pipeline

1. **Query Routing**: LLM-based intent detection extracts: 
   - Target ticker (e.g., "LVMH" â†’ `MC.PA`)
   - Time window (e.g., "last week" â†’ start/end timestamps)
   - Intent type (REAL_TIME vs HISTORICAL)

2. **Semantic Search**: ChromaDB query with:
   - Vector similarity (sentence-transformers embeddings)
   - Time filtering (timestamp range)
   - Ticker filtering (if specified)

3. **Re-Ranking**: Score calculation based on:
   - Semantic similarity (cosine distance)
   - Time decay (for real-time queries)
   - Formula: `score = similarity * 0.6 + time_decay * 0.4`

4. **Context Building**: Top 8 documents formatted with:
   - Document type indicators (ðŸ“Š metrics, ðŸ“ˆ technical, ðŸ“° news)
   - Timestamps and metadata
   - Sentiment scores

5. **LLM Generation**: Llama 3.3 70B produces:
   - Executive verdict
   - Macro & fundamental drivers
   - Technical analysis
   - Forward outlook

### Sentiment Analysis

- **Model**: VADER (Valence Aware Dictionary for Sentiment Reasoning)
- **Translation**: French â†’ English via deep-translator
- **Score Range**: -1 (negative) to +1 (positive)
- **Thresholds**: > 0.5 (positive), < -0.5 (negative)

### Data Retention

- **ChromaDB**: 30-day retention for daily summaries
- **CSV Files**: Unlimited (manual cleanup required)
- **Deduplication**: ID-based for news, date-based for summaries

---

## Troubleshooting

### Kafka Connection Issues

```bash
# Check if Kafka is running
docker compose ps

# View Kafka logs
docker compose logs kafka

# Restart infrastructure
docker compose down && docker compose up -d
```

### Consumer Not Processing

```bash
# Check consumer heartbeat
cat data/history/consumer_heartbeat.txt

# Verify ChromaDB collection
python -c "import chromadb; c=chromadb.PersistentClient('./data/chromadb'); print(c.list_collections())"
```

### Missing Data in Dashboard

1. Wait 10+ minutes for initial data backfill
2. Check producer logs for API errors
3. Verify `.env` file contains valid GROQ_API_KEY

### Groq API Errors

- Verify API key at [console.groq.com](https://console.groq.com)
- Check rate limits (free tier:  30 requests/minute)
- Monitor for model availability issues

---

## Dependencies

| Package | Purpose |
|---------|---------|
| `kafka-python` | Kafka client |
| `yfinance` | Yahoo Finance API |
| `feedparser` | RSS parsing |
| `chromadb` | Vector database |
| `sentence-transformers` | Text embeddings |
| `streamlit` | Web dashboard |
| `groq` | LLM API client |
| `vaderSentiment` | Sentiment analysis |
| `deep-translator` | Text translation |
| `pandas` | Data manipulation |
| `plotly` | Interactive charts |
| `python-dotenv` | Environment variables |
| `watchdog` | File monitoring |

---

