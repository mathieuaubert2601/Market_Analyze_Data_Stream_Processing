import os
import chromadb
import datetime
import sys
from groq import Groq
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv
from collections import Counter

# Ajout du chemin racine pour les imports
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))
from src.config import CHROMA_PATH, COLLECTION_NAME, EMBEDDING_MODEL_NAME, LLM_MODEL_NAME, TICKERS

# --- INITIALISATION ---
load_dotenv()

api_key = os.getenv("GROQ_API_KEY")
if not api_key:
    raise ValueError("‚ùå Cl√© API GROQ manquante dans le fichier .env")

client_groq = Groq(api_key=api_key)

print("‚è≥ [RAG] Chargement du mod√®le d'embedding...")
embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)

print("üìÇ [RAG] Connexion √† ChromaDB...")
chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)
collection = chroma_client.get_collection(name=COLLECTION_NAME)

def get_answer(user_query):
    """
    Fonction principale du RAG :
    Structure la r√©ponse en 3 parties : Pr√©sentation, News/Impact, Technique.
    """
    
    # 1. Vectorisation de la question
    query_vector = embedding_model.encode(user_query).tolist()
    
    # 2. D√âTECTION INTELLIGENTE DU TICKER
    target_ticker = None
    query_upper = user_query.upper()
    
    # Dictionnaire de synonymes (Version compacte)
    SYNONYMS = {
        "STLAP.PA": ["STELLANTIS", "STLA", "PEUGEOT", "PSA", "CITROEN", "FIAT", "CHRYSLER", "FCA", "JEEP"],
        "STMPA.PA": ["STMICROELECTRONICS", "STM", "STMICRO", "SGS-THOMSON", "SEMI-CONDUCTEURS"],
        "ORA.PA":   ["ORANGE", "FRANCE TELECOM", "OPERATEUR"],
        "ENGI.PA":  ["ENGIE", "GDF", "GDF SUEZ", "GAZ DE FRANCE"],
        "ALHPI.PA": ["HOPIUM", "MACHINA", "HYDROGENE"],
        "CS.PA":    ["AXA", "GROUPE AXA", "ASSURANCE"],
        "DCAM.PA":  ["AMUNDI", "ETF MONDE", "MSCI WORLD", "CW8", "AMUNDI WORLD"],
        "ETZ.PA":   ["BNP", "STOXX 600", "ETF EUROPE", "BNP EASY"],
    }

    # Recherche invers√©e dans les synonymes
    for ticker, mots_cles in SYNONYMS.items():
        if any(mot in query_upper for mot in mots_cles):
            target_ticker = ticker
            break
            
    if not target_ticker:
        for t in TICKERS:
            if t in query_upper:
                target_ticker = t
                break
    
    # 3. STRAT√âGIE DE R√âCUP√âRATION (HYBRID FETCHING)
    combined_docs = []
    seen_hashes = set()

    def process_results(results_obj):
        if results_obj['documents']:
            for i, doc in enumerate(results_obj['documents'][0]):
                meta = results_obj['metadatas'][0][i]
                unique_hash = f"{meta.get('ticker')}_{meta.get('doc', '')[:20]}"
                if unique_hash in seen_hashes: continue
                seen_hashes.add(unique_hash)

                try: ts = float(meta.get('timestamp', 0))
                except: ts = 0.0
                
                combined_docs.append({
                    "doc": doc, "meta": meta, "timestamp": ts, 
                    "sentiment": meta.get('sentiment', 0.0)
                })

    # Construction des filtres avec la syntaxe $and
    if target_ticker:
        print(f"üéØ Cible verrouill√©e : {target_ticker}")
        search_filters_news = {"$and": [{"type": {"$eq": "news"}}, {"ticker": {"$eq": target_ticker}}]}
        search_filters_tech = {"$and": [{"type": {"$eq": "technical"}}, {"ticker": {"$eq": target_ticker}}]}
    else:
        search_filters_news = {"type": "news"}
        search_filters_tech = {"type": "technical"}

    # A. R√©cup√©rer les News
    try:
        results_news = collection.query(
            query_embeddings=[query_vector],
            n_results=6,
            where=search_filters_news
        )
        process_results(results_news)
    except: pass

    # B. R√©cup√©rer la Tech
    try:
        results_tech = collection.query(
            query_embeddings=[query_vector],
            n_results=2,
            where=search_filters_tech
        )
        process_results(results_tech)
    except: pass

    # 4. TRI ET S√âLECTION FINALE
    combined_docs.sort(key=lambda x: x['timestamp'], reverse=True)
    
    dominant_ticker = target_ticker
    if not dominant_ticker and combined_docs:
        tickers_found = [d['meta'].get('ticker') for d in combined_docs]
        if tickers_found:
            dominant_ticker = Counter(tickers_found).most_common(1)[0][0]

    # 5. CONSTRUCTION DU CONTEXTE
    context_text = ""
    sources = []
    
    # Si vide
    if not combined_docs:
        return "‚ö†Ô∏è Je n'ai trouv√© aucune information r√©cente (News ou Analyse) pour cette demande dans le flux Kafka.", [], None

    for item in combined_docs[:8]:
        meta = item['meta']
        try: date_str = datetime.datetime.fromtimestamp(item['timestamp']).strftime('%d/%m %H:%M')
        except: date_str = "?"
        
        badge = "ACTUALIT√â" if meta.get('type') == 'news' else "TECHNIQUE"
        ticker_name = meta.get('ticker', 'Inconnu')
        
        context_text += f"SOURCE [{date_str}] ({badge}) SUJET:{ticker_name} CONTENU: {item['doc']}\n"
        
        sources.append({
            "ticker": ticker_name,
            "title": item['doc'],
            "link": meta.get('link', '#'),
            "date": date_str,
            "type": meta.get('type'),
            "sentiment": item['sentiment'],
            "current_price": meta.get('current_price', None),
            "mean_200": meta.get('mean_200', None),
            "mean_50": meta.get('mean_50', None),
            "mean_10": meta.get('mean_10', None),
        })

    # 6. PROMPT STRUCTUR√â (C'EST ICI QUE TOUT CHANGE)
    system_prompt = (
        "Tu es un analyste financier senior de haut niveau."
        "\n\nOBJECTIF :"
        "\nProduire une note d'analyse structur√©e, claire et professionnelle pour un investisseur."
        "\n\nSTRUCTURE DE LA R√âPONSE OBLIGATOIRE :"
        "\n\n1. üè¢ PR√âSENTATION & SECTEUR"
        "\n   - Pr√©sente bri√®vement l'entreprise et son secteur d'activit√©."
        "\n   - *Exception :* Pour cette partie uniquement, tu peux utiliser tes connaissances g√©n√©rales."
        "\n\n2. üì∞ DERNI√àRES ACTUALIT√âS & IMPACT"
        "\n   - R√©sume les actualit√©s fournies dans le CONTEXTE ci-dessous."
        "\n   - Pour chaque news, donne le nom de l'article et analyse bri√®vement son impact potentiel (Positif/N√©gatif/Neutre)."
        "\n   - Si le contexte ne contient aucune news, √©cris : 'Aucune actualit√© r√©cente d√©tect√©e dans le flux'."
        "\n   - *R√®gle :* Utilise STRICTEMENT le contexte. N'invente pas de news."
        "\n\n3. üìà ANALYSE TECHNIQUE DU COURS"
        "\n   - Donne le Prix Actuel et la Variation."
        "\n   - Analyse la Tendance (Haussi√®re/Baissi√®re) en te basant sur les Moyennes Mobiles du contexte."
        "\n   - *R√®gle :* Utilise STRICTEMENT le contexte pour les chiffres."
        "\n\n4. üìù CONCLUSION"
        "\n   - Synth√®se rapide en une phrase sur le sentiment g√©n√©ral (Bullish/Bearish)."
        "\n\nCONTEXTE TEMPS R√âEL (KAFKA) :"
        f"\n{context_text}"
    )
    
    try:
        chat = client_groq.chat.completions.create(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_query}
            ],
            model=LLM_MODEL_NAME,
            temperature=0.2
        )
        return chat.choices[0].message.content, sources, dominant_ticker
    except Exception as e:
        return f"‚ùå Erreur IA : {e}", sources, None